\section{Sparse signal recovery}
\label{sec:algo}
Of course, the major challenge is that we do \emph{not} know the bin-index vector. In this section, we describe our algorithm to recover both $\mathbf{x^*}$ and $\mathbf{p^*}$, given $\mathbf{y, A, s, R}$.  We call our algorithm \emph{MoRAM}, short for \emph{Modulo Reconstruction with Alternating Minimization}. Our approach comprises of two steps: (i) an initialization step, and (ii) descent step via alternating minimization.

\subsection{Initialization by re-calculating the measurements}
\label{sec:init}

Similar to other non-convex approaches, MoRAM also requires an initial estimate $\mathbf{{x}^0}$ that is close to the true signal $\mathbf{{x}^*}$. We have several initialization techniques available; in phase retrieval, techniques such as spectral initialization are often used. However, the nature of the problem in our case is fundamentally different due to the non-linear \emph{additive} behavior of the modulo transfer function. To overcome this issue, we propose a method to re-calculate the true Gaussian measurements ($\mb{y_c}= \mb{Ax^*}$) from the available modulo measurements. 

The high level idea is to undo the nonlinear effect of modulo operation in a significant fraction of the total available measurements. %To understand the method for such re-calculation, we will first try to understand the effect of modulo operation on the linear measurements.

\subsubsection{Effect of the modulo transfer function} 
\label{sec:modeff}
To provide some intuition, let us first examine the distribution of the $\mathbf{Ax^*}$(Fig.~\ref{fig:hist1}) and $\mathbf{\mod(\mathbf{Ax^*})}$ (shown in Fig.~\ref{fig:hist2}) to understand what information can be obtained from the modulo measurements. We are particularly interested in the case where the elements of $\mathbf{Ax^*}$ are small compared to the modulo range parameter $R$. 

Denote the spread (range) of the linear measurements (entries of $\mathbf{Ax^*}$) with the hyper-parameter $\rho > 0$. We choose a value of $\rho$ such that it bounds the maximum element of $|\mathbf{Ax^*}|$. As shown in Fig.~\ref{fig:hist1}, assuming that the entries of $\mathbf{A}$ are Gaussian, $\mathbf{Ax^*}$ lies within $[-\rho, \rho]$ where $\rho$ can be calculated based on Gaussian tail bounds.

In Fig.~\ref{fig:hist2}, we observe the density plots after passing the linear measurements through modulo operation, assuming that $R>\rho$. Comparing these plots with the distribution of true measurements (Fig.~\ref{fig:hist1}), we can observe how the first density plot transforms into the second when the modulo operation is applied. We can draw the following conclusions:

\begin{itemize}
	\item Only those measurement values lying on the negative side of the x-axis are going to be affected.
	\item Measurement values lying very close to the origin on the negative side of the x-axis in the first density plot would now shift rightwards by $R$, and would occupy values very close to $R$ in the second plot. For $R>\rho$, this region is shaded in green in Fig.~\ref{fig:hist2}. The correct bin-index for the elements in $\mathbf{y}$ with value lying between $\rho$ and $R$ is given by $p^{init}_{i} = 1$.
	\item For $R>\rho$, the density plot of the positive region very close to the origin would remain unaffected. This region is shaded with orange color in Fig.~\ref{fig:hist2}. The correct bin-index for all the elements in $\mathbf{y}$ with value lying between $0$ and $R-\rho$ is given by $p^{init}_{i} = 0$.
	\item Nothing can be immediately concluded for measurements lying in between the above mentioned ranges. This region is shaded in gray in Fig.~\ref{fig:hist2}. Correct bin-index cannot be identified for this region, so we assign all the elements in $\mathbf{y}$ with value lying between $0$ and $(R-\rho)$ as $p^{init}_{i} = 0$. The lower and upper bounds ($t_l$~\&$~t_u$) of this region of uncertainty can be obtained as:
	\begin{align*}
	t_l & = R-\rho, \\
	t_u & = R.
	\end{align*}
	%\item Irrespective of relationship between $\rho$ and $R$, all the values lying in the negative half of the real line have the bin-index equal to $1$, and all the values greater than $R$ have the bin-index equal to $0$.
\end{itemize}

\begin{algorithm}[!t]
	\caption{\textsc{MoRAM-initialization}}
	\label{alg:RCM}
	\begin{algorithmic}
		\State\textbf{Inputs:} $\mathbf{y}$, $\mathbf{A}$, $s$, $R$, $\rho$
		\State\textbf{Output:}  $\mb{x^0}$
		\State $U \leftarrow \emptyset$%, $t_l \leftarrow (R-\rho)$, $t_u \leftarrow R$
		\For {$i= 0:m$}
		%		\If {$y_l<0$}
		%		\State {$p^{rcm}_l = 1$, $T \leftarrow T \cup {l}$}
		%		\ElsIf {$0\leq y_l < t_l$}
		%		\State {$p^{rcm}_l = 0$, $T \leftarrow T \cup {l}$}
		%		\ElsIf {$t_l\leq y_l < t_u$}
		%		\State {$p^{rcm}_l = 0$}
		%		\ElsIf {$t_u \leq y_l < R$}
		%		\State {$p^{rcm}_l = 1$, $T \leftarrow T \cup {l}$}
		%		\ElsIf {$R  \leq y_l$}
		%		\State {$p^{rcm}_l = 0$, $T \leftarrow T \cup {l}$}
		%		
		%		\EndIf
		
		\If {$(R-\rho) > y_i$ or $ y_i \geq \rho$}
		\State {$U \leftarrow U \cup \{i\}$}
		\EndIf
		\State Calculate $p^{init}_i$ according to Eq.~\ref{eq:rcm}.
		\EndFor
		\State $N \leftarrow |U|$, calculate $\mb{y^{init}_c}$ according to Eq.~\ref{eq:init}:
		\State $$\mb{x^0} \leftarrow H_s\left( \frac{1}{N}\sum_{i=1}^{N}y^{init}_{c,U,i}a_{U,i}\right)$$
	\end{algorithmic}
\end{algorithm}


\begin{figure}[!t]
	\begin{center}
		\begin{tikzpicture}[scale=0.7, every node/.style={scale=0.7}]
		\def\normaltwo{\x,{3*1/exp(((\x)^2)/2)}}
		\def\y{4.4}
		
		%\fill [fill=orange!60] (2.6,0) -- plot[domain=0:4.4] (\normaltwo) -- ({\y},0) -- cycle;
		
		% Draw and label normal distribution function
		\draw[color=blue,domain=-4.25:4.25,thick] plot (\normaltwo) node[right] {};
		\draw[<->] (-5,0) -- (5,0) node[right] {$\mathbf{Ax^*}$};
		\draw[<->] (0,-1) -- (0,4);
		
		\draw (-3,-0.5) node(below) {$-\rho$};
		\draw (-4,-0.5) node(below) {$-R$};
		\draw (3,-0.5) node(below) {$\rho$};
		\draw (4,-0.5) node(below) {$R$};
		
		\foreach \x in {-3,-4,3,4}
		{        
			\coordinate (A\x) at ($(0,0)+(\x*1cm,0)$) {};
			\draw ($(A\x)+(0,5pt)$) -- ($(A\x)-(0,5pt)$);
			
		}
		%		\draw (-1.5,-0.5) node(below) {$p_i = 1$};
		%		\draw (0,-1.5) node(right) {$f(t) = \mod(t,R)$};
		%		\draw[scale=0.5,domain=-7:0,smooth,variable=\x,blue, ultra thick] plot ({\x},{\x+4});
		%		\draw[scale=0.5,domain=0:7,smooth,variable=\x,blue, ultra thick]  plot ({\x},{\x});
		\end{tikzpicture}
	\end{center}
	\caption{\emph{Histogram of $\mathbf{Ax^*}$}}
	\label{fig:hist1}
\end{figure}

\begin{figure}[!t]
	\begin{center}
		\begin{tikzpicture}[scale=0.7, every node/.style={scale=0.7}]
		\def\normaltwo{\x,{3*1/exp(((\x)^2)/2)}}
		\def\normalone{\x,{3*1/exp(((\x-4)^2)/2)}}
		\def\normalsum{\x,{3*1/exp(((\x-4)^2)/2)+3*1/exp(((\x)^2)/2)}}
		\def\y{3}
		\def\fy{3*1/exp(((\y-4)^2)/2)}
		\fill [fill=orange!60] (0,0) -- plot[domain=0:1] (\normaltwo) -- (1,0) -- cycle;
		\fill [fill=green!60] (3,0) -- plot[domain=3:4] (\normalone) -- (4,0) -- cycle;
		\fill [fill=gray!30] (1,0) -- plot[domain=1:3] (\normalsum) -- (3,0) -- cycle;
		% Draw and label normal distribution function
		\draw[color=blue,domain=-0:3,dashed,thick] plot (\normaltwo) node[right] {};
		\draw[color=blue,domain=1:4,dashed,thick] plot (\normalone) node[right] {};
		\draw[color=blue,domain=-0:4,thick] plot (\normalsum) node[right] {};
		\draw[<->] (-5,0) -- (5,0) node[right] {$\mathbf{Ax^*}$};
		\draw[<->] (0,-1) -- (0,4);
		\draw[dashed] ({\y},{\fy}) -- ({\y},0);
		\draw[dashed] ({4},{3}) -- ({4},0);
		\draw[dashed] ({1},{\fy}) -- ({1},0);
		\draw (-3,-0.5) node(below) {$-\rho$};
		\draw (-4,-0.5) node(below) {$-R$};
		\draw (3,-0.5) node(below) {$\rho$};
		\draw (4,-0.5) node(below) {$R$};
		\draw (1,-0.5) node(below) {$R-\rho$};
		\draw (0.5,1) node(below) {$p_i=0$};
		\draw (3.5,1) node(below) {$p_i=1$};
		\draw (2,0.5) node(below) {$p_i=0$};
		\foreach \x in {-3,-4,1,3,4}
		{        
			\coordinate (A\x) at ($(0,0)+(\x*1cm,0)$) {};
			\draw ($(A\x)+(0,5pt)$) -- ($(A\x)-(0,5pt)$);
			
		}
		%		\draw (-1.5,-0.5) node(below) {$p_i = 1$};
		%		\draw (0,-1.5) node(right) {$f(t) = \mod(t,R)$};
		%		\draw[scale=0.5,domain=-7:0,smooth,variable=\x,blue, ultra thick] plot ({\x},{\x+4});
		%		\draw[scale=0.5,domain=0:7,smooth,variable=\x,blue, ultra thick]  plot ({\x},{\x});
		\end{tikzpicture}
	\end{center}
	\caption{\emph{Histogram of $\mod(\mathbf{Ax^*})$, $R>\rho$}. Best viewed in color.}
	\label{fig:hist2}
\end{figure}
%
%\begin{figure}[!h]
%	\begin{center}
%		\begin{tikzpicture}[scale=0.7, every node/.style={scale=0.7}]
%		\def\normaltwo{\x,{3*1/exp(((\x)^2)/2)}}
%		\def\normalone{\x,{3*1/exp(((\x-2)^2)/2)}}
%		\def\normalsum{\x,{3*1/exp(((\x-2)^2)/2)+3*1/exp(((\x)^2)/2)}}
%		\def\y{3}
%		\def\fy{3*1/exp(((\y-4)^2)/2)}
%		\fill [fill=orange!60] (-1,0) -- plot[domain=-1:0] (\normalone) -- (0,0) -- cycle;
%		\fill [fill=green!60] (2,0) -- plot[domain=2:3] (\normaltwo) -- (3,0) -- cycle;
%		\fill [fill=gray!30] (0,0) -- plot[domain=0:2] (\normalsum) -- (2,0) -- cycle;
%		% Draw and label normal distribution function
%		\draw[color=blue,domain=-0:3,dashed,thick] plot (\normaltwo) node[right] {};
%		\draw[color=blue,domain=-1:2,dashed,thick] plot (\normalone) node[right] {};
%		\draw[color=blue,domain=-0:2,thick] plot (\normalsum) node[right] {};
%		\draw[<->] (-5,0) -- (5,0) node[right] {$\mathbf{Ax^*}$};
%		\draw[<->] (0,-1) -- (0,4);
%		%\draw[dashed] ({\y},{\fy}) -- ({\y},0);
%		\draw[dashed] ({-1},{2}) -- ({-1},0);
%		\draw[dashed] ({3},{2}) -- ({3},0);
%		%\draw[dashed] ({1},{\fy}) -- ({1},0);
%		\draw (-3,-0.5) node(below) {$-\rho$};
%		\draw (-2,-0.5) node(below) {$-R$};
%		\draw (3,-0.5) node(below) {$\rho$};
%		\draw (2,-0.5) node(below) {$R$};
%		\draw (-1,-0.5) node(below) {$-\rho+R$};
%		\draw (-0.5,1) node(below) {$p_i=0$};
%		\draw (2.5,1) node(below) {$p_i=1$};
%		\draw (1,0.5) node(below) {$p_i=0$};
%		\foreach \x in {-3,-2,-1,3,2}
%		{        
%			\coordinate (A\x) at ($(0,0)+(\x*1cm,0)$) {};
%			\draw ($(A\x)+(0,5pt)$) -- ($(A\x)-(0,5pt)$);
%			
%		}
%		%		\draw (-1.5,-0.5) node(below) {$p_i = 1$};
%		%		\draw (0,-1.5) node(right) {$f(t) = \mod(t,R)$};
%		%		\draw[scale=0.5,domain=-7:0,smooth,variable=\x,blue, ultra thick] plot ({\x},{\x+4});
%		%		\draw[scale=0.5,domain=0:7,smooth,variable=\x,blue, ultra thick]  plot ({\x},{\x});
%		\end{tikzpicture}
%	\end{center}
%	\caption{\emph{Histogram of $\mod(\mathbf{Ax^*})$, $R<\rho$}}
%	\label{fig:hist3}
%\end{figure}
We divide the number line in the following $3$ intervals, and assign the bin-index accordingly:
\begin{equation}
{p}^{init}_{i} = 
\begin{cases}
%1,& \text{if } y_i <0 \\
0,& \text{if } 0\leq y_i < t_l \\
0,& \text{if } t_l\leq y_i < t_u ~~~~ \textnormal{(region of uncertainty)} \\
1,& \text{if } t_u \leq y_i < R \\
%0,& \text{if } R  \leq y_i. \\
\end{cases}
\label{eq:rcm}
\end{equation}
Thus, we can identify the correct bin-index for part of the measurements just by observing their magnitude. We define set $U$ as the set of measurements for which we can identify the bin-index correctly. We introduce $N=: \card(U)$.
\begin{align*}
\implies N & =  m - \card(\textnormal{region of uncertainity}).
\end{align*}
Value of $N$ largely depends on the difference between $\rho$ and $R$.

Once we identify the correct bin-index for part of the measurements, we can re-calculate corrected measurements as,
$$
\mathbf{y_{c} = y + p^{init}R}.
$$
We use these corrected measurements $\mathbf{y_{init}}$ to calculate the initial estimate $\mathbf{{x}^0}$ using the estimation step described in the upcoming section.
\subsubsection{Calculating the initial estimate from the corrected measurements}
In the estimation step, $\mb{x^0}$ is calculated only from the $N$ corrected measurements using first order unbiased estimator. For that, we use the versions of $\mb{y}$ and $A$ truncated to the indices belong to set $U$:
\begin{equation}
\mb{x^0} = H_s\left( \frac{1}{N}\sum_{i=1}^{N}y^{init}_{c,U,i}a_{U,i}\right),
\label{eq:init}
\end{equation}
where $H_s$ denotes the hard thresholding operator that keeps the $s$ largest absolute entries of a vector and sets the other entries to zero.
\subsection{Alternating Minimization}
\label{sec:altmin}
\begin{algorithm}[t]
	\caption{\textsc{MoRAM-descent}}
	\label{alg:MoRAM}
	\begin{algorithmic}
		\State\textbf{Inputs:} $\mathbf{y}$, $\mathbf{A}$, $s$, $R$
		\State\textbf{Output:}  $\mb{x^T}$
		\State $m,n \leftarrow \mathrm{size}(\mathbf{A})$ 
		\State \textbf{Initialization}
		\State $\mathbf{x^0} \leftarrow \textrm{MoRAM-initialization}(\mathbf{y, A})$ 
		\State \textbf{Alternating Minimization}
		\For {$t =0:T$}
		\State $\mathbf{{p}^{t}} \leftarrow \frac{\mathbf{1}-\sgn(\langle \mathbf{A} \cdot \mathbf{x^t} \rangle)}{2}$
		\State $\mathbf{y^t_c} \leftarrow \mathbf{y} - \mathbf{p^t}R$
		\State $\mathbf{{x}^{t+1}}\leftarrow \small{JP(\frac{1}{\sqrt{m}}\begin{bmatrix} \mathbf{A} & \mathbf{I} \end{bmatrix},\frac{1}{\sqrt{m}}\mathbf{y^t_c},[\mathbf{x^t~~p^t}]^\t)}$.
		\EndFor
	\end{algorithmic}
\end{algorithm}

Using Eq.~\ref{eq:init}, we calculate the initial estimate of the signal $\mathbf{{x}^0}$ which is relatively close to the true vector $\mathbf{x^*}$. Starting with $\mathbf{{x}^0}$, we  calculate the estimates of $\mathbf{p}$ and $\mathbf{x}$ in alternating fashion to converge to the original signal $\mathbf{x^*}$. At each iteration of our Alternating Minimization, we use the current estimate of the signal ${\mathbf{x^t}}$ to get the value of the bin-index vector $\mathbf{{p}^t}$ as following:
\begin{equation}
\mathbf{{p}^{t}} = \frac{\mathbf{1}-\sgn(\langle \mathbf{A} \cdot \mathbf{x^t} \rangle)}{2}.
\label{step1}
\end{equation}


Given $\mathbf{x^0}$ is close to $\mathbf{x^*}$, $\mathbf{p^0}$ would also be close to $\mathbf{p^*}$. Ideal way is to calculate the correct compressed measurements $\mathbf{y^t_c}$ using $\mathbf{p^t}$, and use $\mathbf{y^t_c}$ with any popular compressive recovery algorithms such as CoSaMP or basis pursuit to calculate the next estimate $\mathbf{{x}_{t+1}}$. Thus,


$$
\mathbf{y^t_c} = \langle \mathbf{A}\mathbf{x^{t}} \rangle = \mathbf{y} - \mathbf{p^t}R,
$$
$$
\mathbf{{x}^{t+1}} = \argmin_{\mathbf{x} \in \mathcal{M}_s}\norm{\mathbf{Ax} - \mathbf{y^t_c}}_2^2, %~~\mathrm{s.to}~~x^* \in \mathcal{M}_s,
$$

%\begin{equation}
%\implies \mathbf{{x}^{t+1}} = \cosamp(\frac{1}{\sqrt{m}}\mathbf{A},\frac{1}{\sqrt{m}}\mathbf{y^t_c},s,\mathbf{x^t}).
%\label{eq:cosamp}
%\end{equation}


However, it should be noted that even the small error $\mathbf{d^t} = \mathbf{p^t - p^*}$ would reflect heavily in the calculation of $\mathbf{y^t_c}$, as each incorrect bin-index would add a noise of the magnitude $R$ in $\mathbf{y^t_c}$. Experiments suggest that the typical sparse recovery algorithms are not robust enough to cope up with such large errors in $\mathbf{y^t_c}$ \cite{Laska2009}. To tackle this issue, we employ the justice pursuit based formulation which is specifically robust towards grossly corrupted measurements. We consider the fact that the nature of error $\mathbf{d^t}$ is sparse with sparsity $s_{dt}=\norm{\mb{d^t}}_0$; and each erroneous element of $\mathbf{p}$ adds a noise of the magnitude $R$ in $\mathbf{y^t_c}$.

The augmented optimization problem becomes,

$$
\mathbf{{x}^{t+1}}=\argmin_{[\mathbf{x~d}]^\t \in \mathcal{M}_{s+s_{dt}}}\norm{\begin{bmatrix} \mathbf{A} & \mathbf{I} \end{bmatrix} \begin{bmatrix} \mathbf{x} \\ \mathbf{d} \end{bmatrix} - \mathbf{y^t_c}}_2^2, %~~\mathrm{s.to}~~x^* \in \mathcal{M}_s,
$$
%\begin{equation}
%= \cosamp(\frac{1}{\sqrt{m}}\begin{bmatrix} \mathbf{A} & \mathbf{I} \end{bmatrix},\frac{1}{\sqrt{m}}\mathbf{y_c^t},s+s_p,[\mathbf{x^t~~p^t}]^\t).
%\label{eq:robcosamp}
%\end{equation}

 However, the sparsity of $\mathbf{d^t}(s_{dt})$ is unknown, suggesting that the sparse recovery algorithms taking sparsity as a parameter cannot be used here. Thus, as we employ basis pursuit, which doesn't rely on sparsity. The robust formulation of basis pursuit is referred as Justice Pursuit (JP) \cite{Laska2009} in the literature, specified in Eq.~\ref{eq:jp}.
\begin{equation}
\implies \mathbf{{x^{t+1}}} = JP(\frac{1}{\sqrt{m}}\begin{bmatrix} \mathbf{A} & \mathbf{I} \end{bmatrix},\frac{1}{\sqrt{m}}\mathbf{y^t_c},[\mathbf{x^t~~p^t}]^\t).
\label{eq:jp}
\end{equation}
We repeat the steps of bin-index calculation (as in Eq.~\ref{step1}) and sparse recovery (Eq.~\ref{eq:jp}) alternatively for $\mathrm{N}$ iterations. Our algorithm is able to achieve perfect convergence, as supported by the results in the experiments section.

%Thus, we can have two variants of the MoRAM algorithm: (i) MoRAM with CoSaMP, (ii) MoRAM with robust CoSaMP, and (iii) MoRAM with Justice Pursuit. 

%$$
%\norm{\begin{bmatrix} \mathbf{A} & \mathbf{I} \end{bmatrix} \begin{bmatrix} \mathbf{x^*} \\ \mathbf{d} \end{bmatrix} - \mathbf{y}}_2^2.
%$$

%
